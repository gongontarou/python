{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "いらすとや/iryou_jujisya_jimu.png\n",
      "ValueError!\n",
      "いらすとや/job_dmat.png\n",
      "いらすとや/enkaku_iryou_doctor_man.png\n",
      "いらすとや/pet_medical_hiru.png\n",
      "いらすとや/enkaku_iryou_man.png\n",
      "いらすとや/onsa_medical.png\n",
      "ValueError!\n",
      "ValueError!\n",
      "いらすとや/building_money_muryouteigaku_shinryou.png\n",
      "いらすとや/enkaku_iryou_doctor_woman.png\n",
      "いらすとや/medical_corset_koshi.png\n",
      "いらすとや/medical_mr_ar_glass.png\n",
      "いらすとや/enkaku_iryou_woman.png\n",
      "ValueError!\n",
      "ValueError!\n",
      "ValueError!\n",
      "ValueError!\n",
      "ValueError!\n",
      "ValueError!\n"
     ]
    }
   ],
   "source": [
    "#●画像ファイルをダウンロードするための準備\n",
    "# ①-①.ライブラリをインポート\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "# ①-②.出力フォルダを作成\n",
    "output_folder = Path('いらすとや')\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "# ①-③.スクレイピングしたいURLを設定\n",
    "url = 'https://www.irasutoya.com/search?q=医療'\n",
    "# ①-④.画像ページのURLを格納するリストを用意\n",
    "linklist = []\n",
    "#●検索結果ページから画像のリンクを取り出す\n",
    "# ②-①.検索結果ページのhtmlを取得\n",
    "html = requests.get(url).text\n",
    "# ②-②.検索結果ページのオブジェクトを作成\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "# ②-③.画像リンクのタグをすべて取得\n",
    "a_list =soup.select('div.boxmeta.clearfix > h2 > a')\n",
    "# ②-④.画像リンクを1つずつ取り出す\n",
    "for a in a_list:\n",
    "# ②-⑤.画像ページのURLを抽出\n",
    "    link_url = a.attrs['href']\n",
    "# ②-⑥.画像ページのURLをリストに追加\n",
    "    linklist.append(link_url)\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "# ●各画像ページから画像ファイルのURLを特定  \n",
    "# ③-①.画像ページのURLを1つずつ取り出す\n",
    "for page_url in linklist:\n",
    "# ③-②.画像ページのhtmlを取得\n",
    "    page_html = requests.get(page_url).text\n",
    "# ③-③.画像ページのオブジェクトを作成\n",
    "    page_soup = BeautifulSoup(page_html, \"lxml\")\n",
    "# ③-④.画像ファイルのタグをすべて取得\n",
    "    img_list = page_soup.select('div.entry > div > a > img')\n",
    "# ③-⑤.imgタグを1つずつ取り出す\n",
    "    for img in img_list:\n",
    "# ③-⑥.画像ファイルのURLを抽出\n",
    "        img_url = (img.attrs['src'])\n",
    "# ③-⑦.画像ファイルの名前を抽出\n",
    "        filename = re.search(\".*\\/(.*png|.*jpg)$\",img_url)\n",
    "# ③-⑧.保存先のファイルパスを生成\n",
    "        save_path = output_folder.joinpath(filename.group(1))\n",
    "        time.sleep(1.0)\n",
    "# ●画像ファイルのURLからデータをダウンロード\n",
    "        try:\n",
    "# ④-①.画像ファイルのURLからデータを取得\n",
    "            image = requests.get(img_url)\n",
    "# ④-②.保存先のファイルパスにデータを保存\n",
    "            open(save_path, 'wb').write(image.content)\n",
    "# ④-③.保存したファイル名を表示\n",
    "            print(save_path)\n",
    "            time.sleep(1.0)\n",
    "        except ValueError:\n",
    "# ④-④.失敗した場合はエラー表示\n",
    "            print(\"ValueError!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//4.bp.blogspot.com/-c7hR2E6Rf3M/UZSs90eyPPI/AAAAAAAATBs/m0ZgnhLvsZE/s400/doctor_gekai.png\n"
     ]
    }
   ],
   "source": [
    "print (img_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
