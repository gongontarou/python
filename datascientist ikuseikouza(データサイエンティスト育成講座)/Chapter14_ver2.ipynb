{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 データサイエンティスト中級者への道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[14.1 この章の概要](#14.1-この章の概要)**\n",
    "    - [14.1.1 データサイエンティスト中級者への道](#14.1.1-データサイエンティスト中級者への道)\n",
    "    - [14.1.2 深層学習を学ぶための準備](#14.1.2-深層学習を学ぶための準備)\n",
    "    - [14.1.3 Pythonの高速化](#14.1.3-Pythonの高速化)\n",
    "    - [14.1.4 Spark入門](#14.1.4-Spark入門)\n",
    "    - [14.1.5 その他の数学的手法とエンジニアリングツール](#14.1.5-その他の数学的手法とエンジニアリングツール)\n",
    "<br><br>\n",
    "- **[14.2 深層学習を学ぶための準備](#14.2-深層学習を学ぶための準備)**\n",
    "    - [14.2.1 パーセプトロン](#14.2.1-パーセプトロン)\n",
    "    - [14.2.2 ニューラルネットワーク](#14.2.2-ニューラルネットワーク)\n",
    "    - [14.2.3 確率的勾配降下法と誤差逆伝播法](#14.2.3-確率的勾配降下法と誤差逆伝播法)\n",
    "    - [14.2.4 深層学習のパッケージ](#14.2.4-深層学習のパッケージ)\n",
    "<br><br>\n",
    "- **[14.3 Pythonの高速化](#14.3-Pythonの高速化)**\n",
    "    - [14.3.1 高速化するためのツール](#14.3.1-高速化するためのツール)\n",
    "    - [14.3.2 BlazeとDask](#14.3.2-BlazeとDask)\n",
    "    - [14.3.3 並列処理](#14.3.3-並列処理)\n",
    "    - [14.3.4 Numba入門](#14.3.4-Numba入門)\n",
    "    - [14.3.5 Cython入門](#14.3.5-Cython入門)\n",
    "<br><br>\n",
    "- **[14.4 Spark入門](#14.4-Spark入門)**\n",
    "    - [14.4.1 Sparkとは](#14.4.1-Sparkとは)\n",
    "    - [14.4.2 PySpark入門](#14.4.2-PySpark入門)\n",
    "    - [14.4.3 SparkSQL](#14.4.3-SparkSQL)\n",
    "<br><br>\n",
    "- **[14.5 その他の数学的手法とエンジニアリングツール](#14.5-その他の数学的手法とエンジニアリングツール)**\n",
    "    - [14.5.1 数学的手法](#14.5.1-数学的手法)\n",
    "    - [14.5.2 エンジニアリングツール](#14.5.2-エンジニアリングツール)\n",
    "<br><br>\n",
    "- **[14.6 総合問題](#14.6-総合問題)**\n",
    "    - [14.6.1 総合問題1](#14.6.1-総合問題1)\n",
    "    - [14.6.2 総合問題2](#14.6.2-総合問題2)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 この章の概要\n",
    "ゴール：データサイエンティスト中級者になるための様々なアプローチやツールを知る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.1 データサイエンティスト中級者への道\n",
    "キーワード：深層学習、Pythonの高速化、Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章では、データサイエンティスト入門レベルを卒業し、中級者の道に進むための色々な分析の手法やアプローチ、ツールについて紹介します。一部実装もありますが、基礎的な説明や概念の紹介が中心になります。深層学習を学ぶために必要となる基礎知識、Pythonの処理を高速化するためのツール群、膨大なデータをサーバーに分散処理させるためのSparkの紹介など、今後データ分析をする上で身につけておきたいスキルばかりです。はじめは理解しにくい箇所もあるかもしれませんが、このようなアプローチやツールがあるということを知っておくだけでも、今後の学習にきっと役に立つはずです。\n",
    "\n",
    "本章は基本的に紹介のみですので、実装の練習問題は少ししかありませんが、ここで紹介した手法やツール等について本格的に学びたい場合は、参考文献等を読んだり、他の講座で勉強して、どんどんスキルを磨いていってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.2 深層学習を学ぶための準備\n",
    "キーワード：パーセプトロン、勾配降下法、確率的勾配降下法、誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最近注目されている深層学習（ディープラーニング）を学ぶための前準備として、いくつか抑えておきたい基礎概念や実装を紹介します。この講座で学んだ統計知識（最尤法、最小二乗法など）や機械学習の考え方（教師あり学習、教師なし学習、交差検証法、混同行列など）と、ここで学ぶ概念（パーセプトロン、勾配降下法、誤差逆伝播法など）を勉強しておけば、深層学習を学ぶ前の良い準備となるでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.3 Pythonの高速化\n",
    "キーワード：データの前処理、並列処理、JITコンパイラ、Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから、機械学習や深層学習等のモデリングの話から離れて、Pythonの処理を高速化するための方法をいくつか紹介します。モデル作成の前に、データ加工をする必要（いわゆる前処理）が多々ありますが、その前処理の計算時間を改善することも大事です。もちろん、コーディングスキルによって、計算処理時間は異なってくるのですが、そのアルゴリズム作成にも限界があります。ここで紹介するライブラリー等を使うことで、計算コストを下げることができます。\n",
    "\n",
    "一昔前、Pythonはスクリプト言語だから遅いという指摘もありました。ただ、昨今、Cythonや並列処理などの色々なライブラリーが出てきており、下手なCプログラムを書くよりも、Pythonで実装したほうがいいこともあります。すべての高速化処理について紹介することはできませんが、計算時間に課題があったときには、ぜひここで使うライブラリー等を使って計算速度をあげることも検討してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.4 Spark入門\n",
    "キーワード：Spark,RDD,PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この講座のデータベースの章で述べた通り、現代の世の中には大量のデータがあり、そのデータは日々蓄積されています。この講座でもある程度、こういったビッグデータに対応できるようなスキルを身に付けるため、いろいろなツールやアプローチを紹介してきました。ここではさらに、そういった膨大なデータに対して、データの加工から機械学習まで一貫して処理し、複数のサーバーを使った分散処理で計算スピードを上げ、さらにリアルタイムに分析ができるSparkを紹介します。今回は入門ということで、その機能と一部実装をみていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.1.5 その他の数学的手法とエンジニアリングツール\n",
    "キーワード：特になし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章の最後に、今後データ分析業務をやっていく上で、必要となってくるであろうその他の数学的な手法や、エンジニアリングツール等を紹介します。数学的な手法では、実験計画法、MCMCなど、エンジニアリング面では、Linuxやクラウドサービスなど簡単に記載します。もちろん、ここですべてが網羅されているわけではありませんが、ビジネスの現場で使われている手法やツールですので、ぜひ参考にしてください。（ただし、これらの手法やツールにとらわれることなく、課題に対する理解や最適なアプローチは何であるか、常に考えていくことが大事だということは忘れないでください。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 深層学習を学ぶための準備\n",
    "ゴール：深層学習を学ぶための基礎知識をおさえる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.1 パーセプトロン\n",
    "キーワード：パーセプトロン、論理回路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、パーセプトロンについて学びましょう。このパーセプトロンは、複数の値を受け取って処理をし、1つの結果を返します。例として、ANDの論理回路の実装を見てみます。以下のAND関数は、2つの数値0か1がインプットとなって、どちらの入力も1の場合に1を返す関数になっています。実装を見ていただくとわかる通り、ある閾値（theta）とインプット値に対する重み（w1,w2）が設定されて、それらのインプット値と重み付けの演算結果(tmp)がその閾値を超えるかどうかで判定をしています。とてもシンプルな実装ですが、この考え方がベースとなって、のちのニューラルネットワークや深層学習につながっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他には、OR関数やNOT関数なども実装できますので、以下の参考文献などを見て実装してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AND_FUNC(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.7\n",
    "    tmp = x1 * w1 + x2 * w2\n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 かつ 0: 0\n",
      "0 かつ 1: 0\n",
      "1 かつ 0: 0\n",
      "1 かつ 1: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"0 かつ 0:\",AND_FUNC(0,0))\n",
    "print(\"0 かつ 1:\",AND_FUNC(0,1))\n",
    "print(\"1 かつ 0:\",AND_FUNC(1,0))\n",
    "print(\"1 かつ 1:\",AND_FUNC(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**[やってみよう]**\n",
    "\n",
    ">OR関数やNAND(NOT AND)関数はどのように実装しますか。考えて、実装してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装』(斎藤 康毅  (著),オライリージャパン)\n",
    "\n",
    ">『機械学習スタートアップシリーズ これならわかる深層学習入門 (KS情報科学専門書)』(瀧 雅人 (著),講談社)\n",
    "\n",
    ">『深層学習 (機械学習プロフェッショナルシリーズ) 』(岡谷 貴之  (著),講談社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "NOT AND 関数を作成して、それがあっているかどうか確かめてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 解答\n",
    "def NAND_FUNC(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.7\n",
    "    tmp = x1 * w1 + x2 * w2\n",
    "    \n",
    "    # ここの条件分岐が異なる\n",
    "    if tmp >= theta:\n",
    "        return 0\n",
    "    elif tmp < theta:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 かつ 0: 1\n",
      "0 かつ 1: 1\n",
      "1 かつ 0: 1\n",
      "1 かつ 1: 0\n"
     ]
    }
   ],
   "source": [
    "# チェック\n",
    "print(\"0 かつ 0:\",NAND_FUNC(0,0))\n",
    "print(\"0 かつ 1:\",NAND_FUNC(0,1))\n",
    "print(\"1 かつ 0:\",NAND_FUNC(1,0))\n",
    "print(\"1 かつ 1:\",NAND_FUNC(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.2 ニューラルネットワーク\n",
    "キーワード：ニューラルネットワーク、活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "次は、ニューラルネットワークです。ニューラルネットワークは、順伝播型ネットワークやフォワードネットワークともいわれ、層状に並べたユニットが、隣り合った層と結合しており、入力情報から出力情報に一方的に伝播していくモデルです。各入力情報を受け取って重み付けをして、その値から活性化関数と言われる関数を使って、出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下の参照URLを参考にしてください。上側の図では、入力値x1:nがw1:nによって重みづけされ、fを関数として計算をして、出力します。下側の図は、層が複数になっているイメージです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comment](https://thinkit.co.jp/images/article/30/2/3021.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "参照URL:https://thinkit.co.jp/images/article/30/2/3021.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "活性化関数には、ロジスティックシグモイド関数、正規化線形関数等が使われています。詳細は、上で紹介した参考文献『深層学習 (機械学習プロフェッショナルシリーズ) 』(岡谷 貴之  (著),講談社)などを見てください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.3 確率的勾配降下法と誤差逆伝播法\n",
    "キーワード：勾配（降下）法、バッチ学習、ミニバッチ学習、確率的勾配降下法、誤差逆伝播法、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本講座の確率統計の章や機械学習の章で、予測した目的関数の値と実際の値のずれ、すなわち誤差をできる限り小さくするために、誤差関数に最小二乗法を適応するアプローチ等を学んできました。私たちがやりたいのは、この誤差関数を最小化する値（パラメータ）を求めることです。そのアプローチ方法をさらに詳しく紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差関数を最小化したい値を探すとき、簡単な関数（二次関数の放物線など）だと求めやすいのですが、複雑な関数になってくると、どこで最小値を取るかを解析的に求めることが困難なケースもあったりします。そのようなときに、まずはどこかに値をとって、値が小さくなる方向の勾配を求めて最小値を探していくのが**勾配法**です。これを繰り返し計算し、最小値がどこにあるのか探していきます。傾きがその方向を決めるため、数学的には関数の偏微分を求めます。なお、最小値を求めるために下に進めていくことが多いので、勾配降下法といったりもします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "さて、データを使って学習させるのに、訓練データをすべて使って一気に学習をさせる方法を**バッチ学習**といいます。従来はこのアルゴリズムが採用されていました。しかし、今日のデータ状況からみて、膨大なデータをすべて使って学習させるのには、かなりのコスト（時間やお金）がかかります。そこで、訓練データの中から無作為にデータを選んで、そのデータを使って学習をしていく（重み付けしていく）方法である**ミニバッチ学習**が使われています。このアプローチを取れば、大量なデータをすべて処理する必要がなくなり、計算コストを下げることができます。（なお、データ1つ1つを取り出して学習させる**オンライン学習**もありますが、ここでは省略します。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "次に紹介する**確率的勾配降下法**は、このミニバッチ学習（訓練データのサンプルを一部（1つだけでも可））を使って、勾配降下法によりパラメータの更新を行う方法です。すべてのデータを使っていないため、計算効率が向上し、さらに局所的な解になってしまうリスクを抑えることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ただし、改善できるとはいえ、勾配法も計算時間がかかることもありますので、それを解決するために**誤差逆伝播法**があります。これは、重み付けのパラメータの勾配計算を効率良く計算する手法です。逆伝播とは、出力層側から入力層に誤差情報を伝えていくことをいい、誤差逆伝播法はこのアプローチをとります。上記の参照URLの図にて、下の後ろ向き演算がそのイメージです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以上で、概念的な紹介は終わりになります。同じく、詳細を知りたい方は、上で紹介した参考文献『ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装』(斎藤 康毅  (著),オライリージャパン)等を見てください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.4 深層学習のパッケージ\n",
    "キーワード：chainer, theano,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "さて、これまでは深層学習の前学習ということで、パーセプトロンや確率的勾配降下法などを学びました。この講座の後に深層学習を学ぶ方は、今後\n",
    "chainerやtheano、tensorflowという深層学習の計算を実行するためのパッケージを使っていくことになりますので、ここで少し紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Chainer\n",
    "\n",
    "株式会社Preferred Infrastructure/Networksが開発するディープラーニングのフレームワークです。いろいろなニューラルネットワークの構造に対応しており、GPUもサポートしています。詳細は、以下のサイトをご覧ください。\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">http://chainer.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- theano\n",
    "\n",
    "モントリオール大学のBengio教授によって開発されている数値計算をするためのライブラリーです。ディープラーニング自体の実装ではなく、それを計算するためのいろいろなサポート（微分計算、コンパイラ）などをしています。こちらもGPUで使うことができます。\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">http://deeplearning.net/software/theano/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Tensorflow\n",
    "\n",
    "Googleがリリースした機械学習やディープラーニングのライブラリです。様々なOSに対応しています。詳細は以下のURLにありますので、参考にしてください。\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">https://www.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以上で、深層学習を学ぶための準備を終わります。近年は深層学習に関する理論的な本や実装に関する良い本も出ています。以下をお勧めします。この講座を終えられた方ならスムーズに入れるのではないかと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『詳解 ディープラーニング ~TensorFlow・Kerasによる時系列データ処理~』(巣籠 悠輔 (著),マイナビ出版)\n",
    "\n",
    "\n",
    ">『Hands-On Machine Learning With Scikit-Learn and Tensorflow:Concepts, Tools, and Techniques to Build Intelligent Systems\n",
    "』（Aurelien Geron (著),Oreilly & Associates Inc）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "また、ディープラーニングについては以下の本がとても有名ですので、紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『Deep Learning (Adaptive Computation and Machine Learning series)』(Ian Goodfellow  (著),Yoshua Bengio  (著),Aaron Courville  (著)、The MIT Press)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "次は、話題を変えて、Pythonの高速化について学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Pythonの高速化\n",
    "ゴール：Pythonを高速化するための方法を知る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.1 高速化するためのツール\n",
    "キーワード：Blaze、Dask、multiprocessing、Numba、Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ここから、Pythonの処理を高速化するためのツールやアプローチについて学びます。Pythonを高速化するには色々とありますが、ここでは、ビッグデータを扱うためのBlazeや並列処理をするDask、multiprocessing、コンパイラによるNumba、Cython等について説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pythonの高速化については、以下の参考文献があります。1つ目の『ハイパフォーマンスPython』は、全体的なコンピューターシステム視点でPython処理のボトルネックとなる箇所を探すために、プロファイリング（システムをテストして遅い箇所を特定する）を実施したり、テクニカルな視点（リストとタプルの違いと扱い方など）でも説明がされています。2つ目の『科学技術計算のためのPython入門』も高速化について、いくつかの章で解説がされていますので、参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『ハイパフォーマンスPython』(Micha Gorelick (著), Ian Ozsvald (著), 相川 愛三 (翻訳),オライリージャパン)\n",
    "\n",
    ">『科学技術計算のためのPython入門』(中久喜健司,技術評論社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**なお、環境等によって、計算時間が変わってきますので、記載通りの結果にならないこともありますが、ご了承ください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.2 BlazeとDask\n",
    "キーワード：Blaze、Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "まずは、ビッグデータを処理するためのBlazeについて紹介します。色々なデータ蓄積システムからデータを取得するためのインタフェースです。使い方としては、pandasと似ていますが、pandasにはない機能もあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下はデータのサンプルを読み込んでいます。**注：バージョンの依存関係等のためWarningがでますが、2回実行すればWarningは消え、あとの処理は実行できます。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import blaze as bz\n",
    "from blaze import data, by, join, merge, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'name':['a','b','c'],'id':[1,2,3],'score':[30,50,40]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>b</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<'DataFrame' data; _name='_1', dshape='3 * {id: int64, name: ?string, score: int64}'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbz = bz.Data(df)\n",
    "dbz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上記のデータを使って、以下scoreの平均を出しています。pandasも同様のことができました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "40.0"
      ],
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbz[\"score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "他、アスタリスク（＊）をつけて、複数のデータファイルの読み込みもできます。特定の文字があるファイル名を読み込む場合に便利です。2章で使ったstudent-mat.csvとstudent-por.csvのファイルを一緒に読み込んでみましょう。**ファイルがある場所の絶対パスは適宜変更してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_data = data(\"/root/userspace/chapters/chap2/student*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他、詳しいことは以下の参考URLなどを見てみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考URL]\n",
    "\n",
    ">http://blaze.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "次は、daskです。これは、複数コアを使って並列処理をするためのライブラリーです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import time\n",
    "# pandasに並列処理を追加\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "以下のデータを使って、それぞれのカラムの最小値を求めるのに、分散処理をさせています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    4.3\n",
       "sepal_width     2.0\n",
       "petal_length    1.0\n",
       "petal_width     0.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの読み込み\n",
    "iris_data = sklearn.datasets.load_iris()\n",
    "hdf = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "df = pd.DataFrame(iris_data.data, columns=hdf)\n",
    "\n",
    "# 分散処理するための記述\n",
    "div_df = dd.from_pandas(df,4)\n",
    "\n",
    "# 計算実行\n",
    "div_df.min().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『Python言語によるビジネスアナリティクス 実務家のための最適化・統計解析・機械学習』(久保 幹雄  (著), 小林 和博 (著), 斉藤 努 (著), 並木 誠 (著), 橋本 英樹 (著),近代科学社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.3 並列処理\n",
    "キーワード：並列処理、multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次の方法は、multiprocessingの並列処理を使った高速化です。\n",
    "\n",
    "まずは、以下でインプットの値を2乗してそれを表示させ、0.5秒待つ処理をしています。これは特に特別な処理はせず、普通に実装しているだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "CalcTime: 5.036837100982666\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# インプットを二乗して結果を表示し、0.5秒待つ\n",
    "def calc(x):\n",
    "    a = x**2\n",
    "    print(a)\n",
    "    # 1秒待つ\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ここから処理を始める\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 計算開始時間\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # リストデータの作成\n",
    "    data = [t for t in range(0,10)]\n",
    "\n",
    "    # 関数の呼び出しとリスト化\n",
    "    [calc(x) for x in data]\n",
    "    \n",
    "    # 計算時間の測定\n",
    "    print(\"CalcTime:\",time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の実装では、計算時間は約5秒です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、multiprocessingを使って並列処理してみましょう。multiprocessing.Process等を使い、以下の実装をみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n",
      "49\n",
      "25\n",
      "1\n",
      "16\n",
      "36\n",
      "64\n",
      "4\n",
      "81\n",
      "CalcTime: 1.5637259483337402\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "# インプットデータから以下のcalcを使ってリスト化する関数\n",
    "def worker(data):\n",
    "    [calc(x) for x in data]\n",
    "\n",
    "# インプットを二乗して結果を表示し、0.5秒待つ\n",
    "def calc(x):\n",
    "    a = x ** 2\n",
    "    print(a)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ここから処理開始\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # プロセスを分けるための設定\n",
    "    split_data = [[0, 1, 2], [3, 4],[5, 6],[7, 8, 9]]\n",
    "\n",
    "    jobs = []\n",
    "    for data in split_data:\n",
    "        job = multiprocessing.Process(target=worker, args=(data, ))\n",
    "        jobs.append(job)\n",
    "        job.start()\n",
    "\n",
    "    [job.join() for job in jobs]\n",
    "    \n",
    "    print(\"CalcTime:\",time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんと計算時間は約1.5秒に改善しました。\n",
    "\n",
    "次は、マルチスレッドを使った処理を見ていきましょう。以下の処理では3つのウェブアドレスにリクエストを送って、待ち時間の合計を計算しています。こちらは普通の実装です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.252300977706909\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "current_time = time.time()\n",
    "urls = ['http://www.google.com', 'http://www.yahoo.co.jp/', 'https://www.bing.com/']\n",
    "for url in urls:\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "print(time.time() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の結果から、約2.6秒かかりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、マルチスレッドを使って計算してみましょう。threadingを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.google.com: 0.2912478446960449\n",
      "https://www.bing.com/: 0.4851679801940918\n",
      "http://www.yahoo.co.jp/: 0.927203893661499\n",
      "Time: 0.9304249286651611\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import threading, time\n",
    "\n",
    "def get_html(url):\n",
    "    current_time = time.time()\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    print(url + ': ' + str(time.time() - current_time))\n",
    "\n",
    "urls = ['http://www.google.com', 'http://www.yahoo.co.jp/', 'https://www.bing.com/']\n",
    "threads = []\n",
    "\n",
    "# Start Threads\n",
    "current_time = time.time()\n",
    "for url in urls:\n",
    "    thread = threading.Thread(target=get_html, args=(url,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "# Wait Threads\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print('Time: ' + str(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は約1秒になり、先ほどより改善されているのがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参照URL]\n",
    "\n",
    ">http://news.mynavi.jp/series/python/032/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "xを引数（インプット）として、以下の関数\\begin{eqnarray*}\n",
    "\\ x^3 + x^2 + x +1\n",
    "\\end{eqnarray*}を計算して結果を表示し、1秒スリープ（待つ）関数を作成してください。また、そのインプットを0から4まで入れて計算し、結果を表示させてください。さらに、その処理についてmultiprocessingを使って計算時間を改善してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "15\n",
      "40\n",
      "CalcTime(Normal): 4.011596918106079\n",
      "0\n",
      "9\n",
      "1\n",
      "16\n",
      "4\n",
      "CalcTime(Multi): 1.5249249935150146\n"
     ]
    }
   ],
   "source": [
    "# 解答\n",
    "import time\n",
    "\n",
    "# 計算したい関数\n",
    "def calc2(x):\n",
    "    a = x**3 + x**2 + x + 1\n",
    "    print(a)\n",
    "    time.sleep(1)\n",
    "\n",
    "# ここから処理を始める\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ########################### \n",
    "    ##### ここから普通の処理 ##### \n",
    "    ###########################\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # リストデータの作成\n",
    "    data = [t for t in range(0,4)]\n",
    "\n",
    "    # 関数の呼び出しとリスト化\n",
    "    [calc2(x) for x in data]\n",
    "    \n",
    "    # 計算時間の測定\n",
    "    print(\"CalcTime(Normal):\",time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    ##### ここから並列処理 ##### \n",
    "    #########################\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # プロセスを分けるための設定\n",
    "    split_data = [[0, 1, 2], [3, 4]]\n",
    "\n",
    "    jobs = []\n",
    "    for data in split_data:\n",
    "        job = multiprocessing.Process(target=worker, args=(data, ))\n",
    "        jobs.append(job)\n",
    "        job.start()\n",
    "\n",
    "    [job.join() for job in jobs]\n",
    "    \n",
    "    print(\"CalcTime(Multi):\",time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.4 Numba入門\n",
    "キーワード：numba,JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、JITコンパイラを使って、高速化しましょう。numbaを使います。使い方は高速化したい関数の前にデコレータ（入力として関数を受け取り別の関数を返す）の@jitをつけます。これにより、JITコンパイラが機械語にコンパイルしてくれて、Cにも匹敵する計算性能が出せるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# jit\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#それぞれの要素の掛け算を実行して、リストにする\n",
    "#普通の実装\n",
    "def mult_abs_basic(N,x,y):\n",
    "    r = []\n",
    "    for i in range(N):\n",
    "        r.append(abs(x[i] * y[i]))\n",
    "    return r\n",
    "\n",
    "#numpyの実装\n",
    "def mult_abs_numpy(x,y):\n",
    "    return np.abs(x*y)\n",
    "\n",
    "#JITの実装\n",
    "@jit('f8[:](i8, c16[:], c16[:])',nopython=True)\n",
    "def mult_abs_numba(N,x,y):\n",
    "    r = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        r[i] = abs(x[i] * y[i])\n",
    "    return r\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は計算をするための変数の準備をします。なお、以下のJは複素数を扱うために使用しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_np = (np.random.rand(N)-0.5) + 1J*(np.random.rand(N)-0.5)\n",
    "y_np = (np.random.rand(N)-0.5) + 1J*(np.random.rand(N)-0.5)\n",
    "\n",
    "x = list(x_np)\n",
    "y = list(y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではまず、普通に計算した場合の時間を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc Time:0.318[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "b1 = mult_abs_basic(N,x,y)\n",
    "print('Calc Time:{0:0.3f}[s]'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は約0.3秒でした。次は、numpyを使って計算した場合です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc Time:0.047[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "b1 = mult_abs_numpy(x_np,y_np)\n",
    "print('Calc Time:{0:0.3f}[s]'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は、約0.044秒でした。以前学んだ通り、numpyを使った方が計算時間が大幅に改善されているのがわかります。\n",
    "\n",
    "最後にJITを使って計算した時を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc Time:0.010[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "b1 = mult_abs_numba(N,x_np,y_np)\n",
    "print('Calc Time:{0:0.3f}[s]'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は0.01秒になっています。Cで実装されているNumpyよりもかなり計算時間が削減されていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『科学技術計算のためのPython入門』(中久喜健司,技術評論社)\n",
    "\n",
    ">http://qiita.com/kenmatsu4/items/7c08a85e41741e95b9ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.5 Cython入門\n",
    "キーワード：Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonの高速化の最後にCythonを使った実装例を紹介します。以下のようにマジックコマンドを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は、以前扱った素数とフィボナッチ数を計算する実装です。はじめは、普通の処理から見ていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 性能比較用　Python関数\n",
    "def pyfib(n):\n",
    "    a, b = 0.0, 1.0\n",
    "    for i in range(n):\n",
    "        a, b = a+b, a\n",
    "    return a\n",
    "\n",
    "def pyprimes(kmax):\n",
    "    p = np.zeros(1000)\n",
    "    result = []\n",
    "\n",
    "    # 最大個数は1000個\n",
    "    if kmax > 1000:\n",
    "        kmax = 1000\n",
    "\n",
    "    k = 0\n",
    "    n = 2\n",
    "    while k < kmax:\n",
    "        i = 0\n",
    "        while i < k and n % p[i] != 0:\n",
    "            i += 1\n",
    "\n",
    "        if i == k:\n",
    "            p[k] = n\n",
    "            k += 1\n",
    "            result.append(n)\n",
    "        n += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、Cythonのコードを書いています。変数の前に型が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython -n test_cython_code\n",
    "def fib(int n):\n",
    "    cdef int i\n",
    "    cdef double a=0.0, b=1.0\n",
    "\n",
    "    for i in range(n):\n",
    "        a, b = a+b, a\n",
    "    return a\n",
    "\n",
    "def primes(int kmax):\n",
    "    cdef int n, k, i\n",
    "    cdef int p[1000]\n",
    "    result = []\n",
    "\n",
    "    if kmax > 1000:\n",
    "        kmax = 1000\n",
    "\n",
    "    k = 0\n",
    "    n = 2\n",
    "    while k < kmax:\n",
    "        i = 0\n",
    "        while i < k and n % p[i] != 0:\n",
    "            i += 1\n",
    "\n",
    "        if i == k:\n",
    "            p[k] = n\n",
    "            k += 1\n",
    "            result.append(n)\n",
    "        n += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、普通のコードとCythonのコードを比較してみましょう。まずは、フィボナッチ数列からです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 loops, best of 3: 1.06 µs per loop\n",
      "10000 loops, best of 3: 54.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "# フィボナッチ計算比較\n",
    "%timeit fib(1000)\n",
    "%timeit pyfib(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間がなんと約50倍も改善しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 2.05 ms per loop\n",
      "1 loop, best of 3: 290 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# 素数計算比較\n",
    "%timeit primes(1000)\n",
    "%timeit pyprimes(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらは100倍以上の改善となりました。\n",
    "\n",
    "以上で、Pythonの高速化は終わりになります。計算に時間がかかる時は、上記のような手法を用いて、高速化できるかどうか検討して試してみてください。\n",
    "\n",
    "次は、Sparkになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『Cython ―Cとの融合によるPythonの高速化』(Kurt W. Smith (著), 中田 秀基 (監修), 長尾 高弘  (翻訳),オライリージャパン )\n",
    "\n",
    ">http://qiita.com/kenmatsu4/items/7c08a85e41741e95b9ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Spark入門\n",
    "ゴール：Sparkの機能について知る、PySparkの基礎的な機能を使える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.1 Sparkとは\n",
    "キーワード：Spark SQL, Spark Streaming, MLib, GraphX, PySpark,Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "冒頭で述べた通りSparkは分散処理をするためのツールで、機械学習のライブラリーも使え、しかもSQLのように簡単に集計ができたり、リアルタイムに分析できます。ビッグデータを解析するためのソフトウェアの1つです。SparkはScalaベースで作られていますが、Scala以外にもPythonやJavaなどからも利用可能です。本講座では、Pythonとの連携をメインとして、その処理方法の基礎を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.2 PySpark入門\n",
    "キーワード：RDD, key/value, mapreduce, sparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、PythonとSparkを連携させたPySparkの使い方について、基礎の基礎を学びましょう。まずは、この環境でPySparkを使うために、以下のコードを実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコマンドで、spark(pyspark)が使えることを確認しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1023e8668>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、分析対象のデータの読み込みをします。sc.textFileの後にデータのある場所を指定しています。なお、ファイル名にアスタリスク(✴︎)をつけて任意の文字を含むファイル名をまとめて読みこむことが可能です。同じような規則性のあるファイル名を扱う時は便利です。確率と統計の章で扱った「student-mat.csv」と「student-por.csv」を同時に読み込んでみましょう。ただし、ここでは重複等は気にせず、そのまま読み込みます。**なお、該当のデータがある場所は絶対パスで指定してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_student_data = sc.textFile(\"/root/userspace/chapters/chap2/student*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはRDD（Resilient Distributed Dataset）としてデータをロードさせています。日本語では、「不変・並列実行可能な(分割された)コレクション」という意味です。RDDをベースにSparkはデータを分散処理させます。Sparkのすべての作業は、このようなRDDの生成、既存のRDDの変換、結果を集計するためにRDDに対する呼び出しをしています。この段階ではまだ集計等はしていません。\n",
    "\n",
    "次は、行数を数えています。ここで集計を開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Line count\n",
    "merge_student_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下でデータのはじめの行を表示します。RDDの後にfirst()をつけて実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_student_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take(5)で5行を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3',\n",
       " '\"GP\";\"F\";18;\"U\";\"GT3\";\"A\";4;4;\"at_home\";\"teacher\";\"course\";\"mother\";2;2;0;\"yes\";\"no\";\"no\";\"no\";\"yes\";\"yes\";\"no\";\"no\";4;3;4;1;1;3;6;\"5\";\"6\";6',\n",
       " '\"GP\";\"F\";17;\"U\";\"GT3\";\"T\";1;1;\"at_home\";\"other\";\"course\";\"father\";1;2;0;\"no\";\"yes\";\"no\";\"no\";\"no\";\"yes\";\"yes\";\"no\";5;3;3;1;1;3;4;\"5\";\"5\";6',\n",
       " '\"GP\";\"F\";15;\"U\";\"LE3\";\"T\";1;1;\"at_home\";\"other\";\"other\";\"mother\";1;2;3;\"yes\";\"no\";\"yes\";\"no\";\"yes\";\"yes\";\"yes\";\"no\";4;3;2;2;3;3;10;\"7\";\"8\";10',\n",
       " '\"GP\";\"F\";15;\"U\";\"GT3\";\"T\";4;2;\"health\";\"services\";\"home\";\"mother\";1;3;0;\"no\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";3;2;2;1;1;5;2;\"15\";\"14\";15']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_student_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、lambdaも組み合わせて、フィルターをします。具体的には、それぞれのラインについて、GPという文字が含まれているものを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gp_lines = merge_student_data.filter(lambda line: \"GP\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補足ですが、まだ上のフィルターの段階では、計算はしておらず、上記のcountやfirstを実行した時に計算がされます。これはSparkの**遅延評価**によるものです。先ほどの、take()も実施した時に、計算がされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果より、GPが含まれるレコード数は772であることがわかりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、先頭のカラムがあるレコードをカウントしましょう。先ほど2ファイルをそのまま読み込んでいるので、先頭のカラムは2行あるはずです。以下は、カラム名schoolが含まれているレコード数をカウントしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head_lines = merge_student_data.filter(lambda line: \"school\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを応用して、MapReduceの処理を見てみましょう。以下では、それぞれのセルに入っているデータ（数字も）を1つの単語と見なし、それぞれいくつあるのかをカウントする処理をしています。\n",
    "\n",
    "まず、splitを使って「;」で文字を分けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_words = merge_student_data.flatMap(lambda line:line.split(\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、mapで単語1つにカウント1を対応させ、reduceの処理でそれぞれ同じ単語のカウントを足しあげています。これがMapReduceの処理です。キーとなっているのが単語(word)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = split_words.map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect()でRDD全体を取り出しますので、大きなデータを扱う際は注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = word_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返ってくる値はリスト型です。数が多いので表示結果は絞っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('24', 2),\n",
       " ('schoolsup', 2),\n",
       " ('\"health\"', 123),\n",
       " ('30', 2),\n",
       " ('Fjob', 2),\n",
       " ('22', 7),\n",
       " ('\"U\"', 759),\n",
       " ('\"4\"', 4),\n",
       " ('G1', 2),\n",
       " ('16', 350),\n",
       " ('54', 1),\n",
       " ('higher', 2),\n",
       " ('G3', 2),\n",
       " ('56', 1),\n",
       " ('\"M\"', 453),\n",
       " ('40', 1),\n",
       " ('romantic', 2),\n",
       " ('\"GP\"', 772),\n",
       " ('internet', 2),\n",
       " ('13', 117)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、Statisticsを使うことで、統計量の計算も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([Vectors.dense([2, 0, 0, -2]),Vectors.dense([4, 5, 0,  3]),Vectors.dense([6, 7, 0,  8])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、 Statistics.colStatsを使って、基本統計量を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = Statistics.colStats(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  4.  0.  3.]\n",
      "[  4.  13.   0.  25.]\n",
      "[ 3.  2.  0.  3.]\n"
     ]
    }
   ],
   "source": [
    "print (summary.mean())\n",
    "print (summary.variance())\n",
    "print (summary.numNonzeros())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、データの読み込みと簡単な集計の紹介は終わります。次は、SparkSQLを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "上のデータmerge_student_dataに対して、schoolsupを含むレコードがどれだけあるかカウントしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 解答\n",
    "schoolsup_lines = merge_student_data.filter(lambda line: \"schoolsup\" in line)\n",
    "schoolsup_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.3 SparkSQL\n",
    "キーワード：SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkはSQLも扱うことができます。以下は、そのモジュールの読み込みと準備をしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,FloatType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率統計の章で扱ったデータを対象に、データベースの章で学んだSQL（SparkSQL）を使って集計してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込みます。RDDとしてロードしています。なお、該当のデータがある場所は絶対パスで指定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student_mat_data = sc.textFile(\"/root/userspace/chapters/chap2/student-mat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ヘッダーを設定するために、ヘッダー部分だけ読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = student_mat_data.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータの区切り文字は特殊でした。以下のように;を置き換えましょう。replaceを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaString = header.replace(';',',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school,sex,age,address,famsize,Pstatus,Medu,Fedu,Mjob,Fjob,reason,guardian,traveltime,studytime,failures,schoolsup,famsup,paid,activities,nursery,higher,internet,romantic,famrel,freetime,goout,Dalc,Walc,health,absences,G1,G2,G3'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、データベースのテーブルを準備するとき、フィールド名や型を全て指定しなければいけませんが、30以上もあるので、設定するのは大変です。以下のようにプログラムを書いて、作業を効率化しましょう。取り急ぎすべて文字型にしています。個別で後で変更も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをデータ構造としてスキーマを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどのカラムの行（ヘッダー）を除く、データを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Header = student_mat_data.filter(lambda l: \"school\" in l)\n",
    "NoHeader = student_mat_data.subtract(Header)\n",
    "NoHeader.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は、カラム名を除いたデータの1行目です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"GP\";\"F\";15;\"R\";\"GT3\";\"T\";1;1;\"at_home\";\"other\";\"home\";\"mother\";2;4;1;\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"no\";3;1;2;1;1;1;2;\"7\";\"10\";10'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoHeader.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また次に、文字が”で囲まれているため、map関数とlambda関数を使って、その文字を消去しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NoHeader2 = NoHeader.map(lambda l: l.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GP;F;15;R;GT3;T;1;1;at_home;other;home;mother;2;4;1;yes;yes;yes;yes;yes;yes;yes;no;3;1;2;1;1;1;2;7;10;10'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoHeader2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、;で文字が区切られているため、分割しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NoHeader3 = NoHeader2.map(lambda l: l.split(\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、上のデータをデータフレームにします。先ほどのスキーマで設定します。sqlContext.createDataFrameを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame = sqlContext.createDataFrame(NoHeader3, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、SQLを使うために、上のデータフレームをテーブルにして、登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame.registerTempTable(\"tmp_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下で、スキーマを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- school: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- famsize: string (nullable = true)\n",
      " |-- Pstatus: string (nullable = true)\n",
      " |-- Medu: string (nullable = true)\n",
      " |-- Fedu: string (nullable = true)\n",
      " |-- Mjob: string (nullable = true)\n",
      " |-- Fjob: string (nullable = true)\n",
      " |-- reason: string (nullable = true)\n",
      " |-- guardian: string (nullable = true)\n",
      " |-- traveltime: string (nullable = true)\n",
      " |-- studytime: string (nullable = true)\n",
      " |-- failures: string (nullable = true)\n",
      " |-- schoolsup: string (nullable = true)\n",
      " |-- famsup: string (nullable = true)\n",
      " |-- paid: string (nullable = true)\n",
      " |-- activities: string (nullable = true)\n",
      " |-- nursery: string (nullable = true)\n",
      " |-- higher: string (nullable = true)\n",
      " |-- internet: string (nullable = true)\n",
      " |-- romantic: string (nullable = true)\n",
      " |-- famrel: string (nullable = true)\n",
      " |-- freetime: string (nullable = true)\n",
      " |-- goout: string (nullable = true)\n",
      " |-- Dalc: string (nullable = true)\n",
      " |-- Walc: string (nullable = true)\n",
      " |-- health: string (nullable = true)\n",
      " |-- absences: string (nullable = true)\n",
      " |-- G1: string (nullable = true)\n",
      " |-- G2: string (nullable = true)\n",
      " |-- G3: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_frame.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、データベースの章で学んだselect文を使ってみましょう。基本的にsqlContext.sqlの中でSQLを記述すれば、大丈夫です。なお、集計結果としては、前半のChapterで実行した結果と同じになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|sex|            avgAge|\n",
      "+---+------------------+\n",
      "|  F| 16.73076923076923|\n",
      "|  M|16.657754010695186|\n",
      "+---+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "count_result = sqlContext.sql(\"select sex,avg(age) as avgAge from tmp_table group by sex\")\n",
    "print(count_result.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、Sparkの説明は終わりになります。Sparkではさらに、前に学んだ機械学習の手法もMLibを使ってRDDに適応させることが可能です。\n",
    "\n",
    "今回は簡単な実装の紹介のみで、ここの環境ではSparkの凄さを実感できませんが、将来的にAWSなどでたくさんのサーバーが使えるときや分散処理を実施する時に、ぜひ使ってみてください。ちなみに、インストール等が少し大変なので、著者はEMRを使っています。ノード数の指定などが簡単に設定できるので、はじめて使う方にはEMRはオススメです。\n",
    "\n",
    "またSparkはScalaベースで作られており、色々な面でScalaからの方が扱いやすいと思いますので、本格的に使われる場合はScalaも選択肢として考えるのも良いと思います（PySparkは制約があります）。\n",
    "\n",
    "なお、参考文献は以下になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『初めてのSpark』(Holden Karau (著), Andy Konwinski (著), Patrick Wendell (著), Matei Zaharia (著), & 1 その他,オライリージャパン )\n",
    "\n",
    ">『入門 PySpark ―PythonとJupyterで活用するSpark 2エコシステム』(omasz Drabas (著),Denny Lee (著),Sky株式会社 玉川 竜司 (翻訳),オライリージャパン )\n",
    "\n",
    ">『Machine Learning with Spark - Tackle Big Data with Powerful Spark Machine Learning Algorithms』(Nick Pentreath (Author),Packt Publishing )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "SparkSQLを使って、先ほど作ったテーブルに対して、school × sexを軸にして、それぞれのレコード数と、それぞれの平均年齢を表示するようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+------------------+\n",
      "|school|sex|cnt|            avgAge|\n",
      "+------+---+---+------------------+\n",
      "|    GP|  F|183|16.579234972677597|\n",
      "|    GP|  M|166|16.457831325301203|\n",
      "|    MS|  F| 25|             17.84|\n",
      "|    MS|  M| 21|18.238095238095237|\n",
      "+------+---+---+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 解答\n",
    "count_result = sqlContext.sql(\"select school,sex,count(*) as cnt,avg(age) as avgAge from \"\n",
    "                              + \"tmp_table group by school,sex order by 1,2\")\n",
    "print(count_result.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 その他の数学的手法とエンジニアリングツール\n",
    "ゴール：データ分析のアプローチ方法(数学的手法とエンジニアリング)を広げる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.5.1 数学的手法\n",
    "キーワード：MCMC, 階層ベイズ、実験計画法、生存解析、確率過程とランダムウォーク、時系列解析、トピックモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この講座では、データ分析、特に機械学習の分野について、最低限必要な手法を学んできました。さらに、いろいろな課題に対するアプローチを増やすために、数学的な手法とそれらのライブラリー等について紹介します。ここでは、ほんの数行程度の説明ですが、この講座終了後に、業務上必要そうなもの、興味があるものをぜひ学んでいってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **マルコフ連鎖モンテカルロ法（MCMC）**：この手法は乱数を使ってマルコフ連鎖（ある状態が直前の状態にのみ依存して、その連鎖を確率モデルで表したもの）を発生させる方法で、多重積分を計算するときなどに使われます。PythonのライブラリーではPyMCから使うことができます。なお、以前に紹介したEMアルゴリズムと、このMCMCの一種であるギブス標本抽出は関係しています（EステップとMステップ）ので、後で紹介する参考文献等をみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **階層ベイズ**：統計モデルにはパラメータ（母平均など）がありましたが、それに階層構造をもたせてベイズ推定する手法が階層ベイズです。単純にパラメータを固定するだけではうまく現象を説明することができないこともあるため、その背後にある確率分布をさらに考えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **実験計画法**：ある現象が生じた場合に、その原因は何なのか、それらには本当に因果関係があるのかどうか、実際の実験を通して観察し、その結果が妥当であるかどうかを分析するのが実験計画法です。実験計画法では少ない実験回数で効果的な情報を得られるように実験計画を工夫します。この講座の前半に、相関があるからといって因果関係があるとはいえないという話をしました。この実験計画法によって、ある現象がある要因によって生じているのかどうかを調べていきます。マーケティング分野ではコンジョイント分析という名で利用されています。具体的な例としては、消費者がパソコンを購入する際、価格や色、デザイン、品質などが購入するかしないかに影響してきますが、それらの影響の度合いを調べるための効率的な実験を、直交表という表を用いて計画します。他には、スーパーなどで発行されるクーポンの効果を見るために、ある店舗群はクーポンを配り（処置群）、別の同じような売り上げの店舗群はクーポンを発行しない（コントロール群）という実験等が行われています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **時系列解析**：時間とともにランダムに変動するデータを分析するのが、時系列解析です。分析対象の主なデータは、気候（気温、雨量など）、株価や地震波、売上推移など様々な時系列データです。それらの移動平均を計算したり、それ自身の過去のデータと相関をとって分析（自己相関）したり、多変量の時系列を解析して、将来の予測計算をします。Pythonのライブラリーでは、statsmodelsがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **確率過程とランダムウォーク、確率解析**：ファイナンス理論（オプション価格のデリバティブ計算等）に応用されることが多く、金融業界でクオンツといわれる職種を目指される方は、これらの領域を学ぶことになります。Pythonでは特に一般的なライブラリー等はないようですが、確率微分方程式の離散化バージョンなどは以下の参考文献などに記載されています。また、参考URL（quantopian）もここと同じように手を動かしながら、金融で必要な実装を学ぶことができ、オススメです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **生存時間解析**：その名の通り、ある分析の対象がどれだけ生き続けているか（人や製品なども含む）を解析するためのアプローチです。生存曲線という、時間と生存する確率を対応させた関数を使いますが、生存時間の分布があらかじめわかっていることは少ないため、それを推定するための手法、カプランマイヤー推定などが使われます。この手法は、あるイベントが発生するまでの時間を解析するための方法で、医療の分野では生存率を評価するときなどに使われているようです。Pythonではlifelinesがパッケージとしてあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **トピックモデル**：自然言語処理の一分野でもあり、ニュースなどたくさんの記事があった時に、それらが一体何のトピックなのか分析するのがトピックモデルです。応用分野としては、ニュース記事の分類以外にも、購買行動の分析（セグメンテーションなど）もあります。Pythonのパッケージでは、gensimがあります。なお自然言語処理は、NLTKパッケージがよく使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上、簡単ではありましたが、その他の数学的な手法の紹介は終わります。他にも、さらなる応用領域（ベイジアンネットワーク、状態空間モデルとカルマンフィルタ、マリアバン解析等）もありますので、興味のある方はいろいろと調べてみてください。\n",
    "\n",
    "特にオススメなのが、以下の参照URLの**quantopian**のウェブサイトです。先ほども少し紹介しました。ファイナンス系のサイトですが、この講座と同様にjupyternotebookを使って手を動かして学べる環境があります。しかも無料です。ファイナンス専門家の方はもちろん、専門外の方もいろいろと学ぶことが多いと思いますので、オススメです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参照URL]\n",
    "\n",
    ">https://www.quantopian.com/home\n",
    "\n",
    ">[参考文献]\n",
    "\n",
    ">『IPythonデータサイエンスクックブック ―対話型コンピューティングと可視化のためのレシピ集』(Cyrille Rossant (著), 菊池 彰 (翻訳),オライリージャパン)\n",
    "\n",
    ">『統計的学習の基礎 ―データマイニング・推論・予測』(Trevor Hastie (著), Robert Tibshirani (著), Jerome Friedman (著), 杉山 将  (翻訳), & 22 その他,共立出版)\n",
    "\n",
    ">『パターン認識と機械学習 上下』(C.M. ビショップ  (著), 元田 浩 (監訳), 栗田 多喜夫  (監訳), 樋口 知之 (監訳), & 2 その他, 丸善出版)\n",
    "\n",
    ">『Pythonで体験するベイズ推論 PyMCによるMCMC入門』(キャメロン デビッドソン=ピロン (著), Cameron Davidson‐Pilon (原著), 玉木 徹  (翻訳),森北出版)\n",
    "\n",
    ">『機械学習スタートアップシリーズ ベイズ推論による機械学習入門 (KS情報科学専門書)』(須山 敦志 (著),杉山 将 (監修)講談社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.5.2 エンジニアリングツール\n",
    "キーワード：Linux, AWS, hadoop, FPGA, AWS,Scala,R, Java, OpenCV, Tableau, PowerBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今までは主にPythonを使ってきましたが、分析に関連するソフトウェアや分析に役立つツールを、まばらではありますが紹介します。ここの講座のみですべてを学ぶことはできませんが、将来的なアンテナを張れるように単語を知っておくだけでもためになると思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Linux（コマンドライン、シェル）**：OSの1つです。この講座でも少しコマンドライン等を使いました。将来、分析をするだけではなく、分析をするための環境構築等にも必要になってくるスキルです。初学者にはなかなかハードルが高いですが、Linuxを徐々に使い慣れていけば、分析環境を構築したり、実際に分析するのに、色々と選択肢が広がるでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Hadoop**：分散処理システムです。よく取り上げられる簡単な例が、ある文章の中に出てくる単語をそれぞれカウントするときにどうやって効率よく数えていくのかという問題です。処理としては、文章の中にある単語に数字を対応させ(Map処理)、後から集めて集計する（Reduce処理）という処理をHadoopがやります（分散処理と監視など）。単語のカウントは、上記のSparkの例でも扱いました。また、Sparkでは、このHadoopエコシステムの中で使われることも多いです。ビックデータ分析といえばHadoopというくらい、大量のデータを処理するのによく紹介されます。ただ、こちらはバッチ処理になりますので、リアルタイムに分析をする場合には、Spark等と組み合わせる必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **FPGA**：昨今、深層学習計算などでたくさんのCPUやGPUが使われており(他、TPUも)、ソフトウェアの面からだけではなく、ハード面で実装をすることが求められています。このFPGAはプログラム可能なICで、Field Programmable Gate Arrayの略です。論理仕様をプログラムすることができ、ユーザーの思い通りの論理回路を実現できる論理デバイスです。メリットとしては、CPUが処理する段階でロジックを組むことができるので、処理スピードがかなり改善されます。ハードウェア記述言語のVHDLなどで書かれており、始めるのにかなりハードルが高いツールですが、最近はPynqなどPythonからも実装ができるようになってきているようです。FPGAは、データ分析中級者向けというより、レベル的にはかなりの上級者（トップクラスのデータサイエンティストかエンジニア）向けだと思いますが、今後この講座を受けた人の中から、データ分析の第一線で活躍している人が出てくることを期待して、紹介をしました。インフラよりの話ではありますが、Pythonからも使えるようになり、分析をするのにCPUレベルで考えなければいけない時代が来るのかもしれません。応用分野としては、MicrosoftのBingの検索エンジンやゲノム科学解析、ゲームユーザーの振り分け、金融の高頻度取引などに使われており、いろいろな企業が注目しています。ただこれも用途やデータに応じて使うものですので、FPGA（やGPUなど）を使えば必ず速くなるというものではないので注意しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **AWS**：Amazonのクラウドサービスです。サーバー構築やデータストレージ環境など様々なサービスが提供されています。試験的に何か始める時には、コストを抑えることができるため便利です。また、一時的に大量のサーバーを立ち上げるなど、分散処理システムを構築したい場合に使いやすいです。HadoopやSparkがあらかじめインストールされるAmazon EMRや、データベースを分散処理により高速化させるRedshiftなど、分析で使われることも多いです。また、Machine Learning（機械学習）などもありますが、現状は一部だけで、ここで学んだ人には物足りないかもしれません。なお、先ほどのFPGAは、2016年12月からAWSでも使えるようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **R**：統計ソフトウェアです。統計的な手法のライブラリーが豊富です。Pythonのパッケージにはないものも、いくつかあります（変数選択法など他、多数）。もしこれらのパッケージをPythonで利用したい場合は、PythonからRのスクリプトも呼び出すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **可視化ツール**：Tableau, PowerBIなどがあります。2章で紹介した通り、近年はインフォグラフィックスというデータの可視化が注目されており、これらのツールを使うことで、簡単にデータを可視化することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **OpenCV**：画像を処理するためのライブラリーです。人の顔を認識したりすることもできます。Pythonと連携が可能で、このJupyterと組み合わせて実行すると便利です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 他：ここで紹介したツールやプログラミング言語の他にも、Java、Rudy、PHP、Scalaや機械学習や統計分析向けのJulia、Jumpも分析用のツールとしてよく使用されているようです。クラウドサービスとしてはAWSの他に、GoogleのGCPやIBMのBlueMix、マイクロソフトのAzureなどがあります。商用の統計解析ツールとしては、SAS、SPSS、Matlabなどがありますが、高額なので一般で買うのは難しいかもしれません。ただ、このような統計ソフトウェアの機能もPythonのライブラリーで提供されているものが多くなってきましたので、ある程度はPythonやRでも実現可能です。Pythonにはない統計手法を使いたい場合や、これらのソフトウェアに興味のある方、大学や会社で使う必要があるという方は、無償版がありますので、それらをインストールしたり、本などをみて勉強してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『ビッグデータ テクノロジー完全ガイド』(Michael Manoochehri (著), 小林 啓倫  (翻訳),マイナビ)\n",
    "\n",
    ">『FPGAの原理と構成』(天野英晴 (編集),オーム社)\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">http://qiita.com/kazunori279/items/a9e97a4463cab7dda8b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、この章は終わります。お疲れ様でした。最後に用語の確認だけしましょう。\n",
    "\n",
    "残りは、総仕上げの演習問題となります。なお、総仕上げの問題は基本的に前までの章の知識をベースにしていますので、余裕がある方は取り組んでみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 総合問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6.1 総合問題1\n",
    "\n",
    "深層学習に関する以下の用語について、それぞれの役割やその意味について述べてください。また、ネットや参考文献等も使って調べてみてください。\n",
    "- パーセプトロン\n",
    "- ニューラルネットワーク\n",
    "- 勾配法\n",
    "- バッチ学習\n",
    "- ミニバッチ学習\n",
    "- 確率的勾配降下法\n",
    "- 誤差逆伝播法\n",
    "- Chainer, Theano, TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6.2 総合問題2\n",
    "\n",
    "Pythonの高速化とSparkに関する用語について、それぞれの役割や意味について述べてください。また、ネットや参考文献等も使って調べてみてください。\n",
    "- 並列処理\n",
    "- JITコンパイル\n",
    "- Cython\n",
    "- Spark\n",
    "- RDD\n",
    "- PySpark\n",
    "- SparkSQL"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
